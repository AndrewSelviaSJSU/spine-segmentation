%! suppress = TooLargeSection
\documentclass[conference]{IEEEtran}
\usepackage{cite}
%\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

% Andrew Selvia added these
\usepackage{subfig}
\usepackage[hidelinks]{hyperref}
\graphicspath{ {images/} }
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}

\newcommand\BibTeX{{\textrm{B} \kern-.05em{\textsc{i} \kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\ker,n-.125emX}}

\begin{document}

    \title{Spine Segmentation using U-Net}

    \author{
    \IEEEauthorblockN{Andrew Selvia (014547273)}
    \IEEEauthorblockA{\textit{Department of Software Engineering}\\
    \textit{San José State University}\\
    San José, California\\
    andrew.selvia@sjsu.edu}
    }

    \maketitle

    \begin{abstract}
        Image segmentation is an important first step of many research projects which apply computer vision techniques to medical imagery.\ This project demonstrates how U-Net~\cite{ronneberger2015unet}, a popular convolutional neural network (CNN) architecture, can be used to isolate the spine from a posteroanterior X-ray.\ Inspired by a similar paper by Horng, Kuok, Fu, Lin, and Sun which performs spine segmentation in service of learning Cobb angles for scoliosis detection~\cite{cobb-angle-measurement-of-spine-from-x-ray-images-using-convolutional-neural-network}, this project replicates the image segmentation task in a novel way while also setting the stage for future exploration of the advanced methods described in that paper.
    \end{abstract}

    \begin{IEEEkeywords}
        machine learning, computer vision, image segmentation, neural networks
    \end{IEEEkeywords}

    \section{Introduction}\label{sec:introduction}

    Scoliosis is a common spinal deformity affecting around 3 percent of the U.S.\ population~\cite{scoliosis-media-and-community-guide}.\ For decades, advocates have promoted testing children starting in grade school to detect spinal curvature as they grow.\ These efforts are justified since no cure exists but early, accurate, and frequent testing can prevent unnecessary suffering later in life.

    Obviously, innovation which might accelerate, standardize, and reduce the cost of scoliosis detection would have a large impact.\ For exactly these reasons, computer vision is a compelling research area which could help millions of people.

    The ultimate goal of my research is to detect Cobb angles from X-rays for scoliosis detection.\ Though that remains out-of-reach for now, I have proven spine segmentation is possible through the use of TensorFlow and the U-Net CNN architecture.

    \section{Implementation}\label{sec:implementation}

    \subsection{Code}\label{subsec:code}

    Similarly to the original paper~\cite{cobb-angle-measurement-of-spine-from-x-ray-images-using-convolutional-neural-network}, U-Net was chosen as the neural network architecture for this project.\ A quick literature study demonstrated its wide adoption for medical imaging tasks, lending confidence that this approach is well-founded.

    The code for this project is hosted on GitHub~\cite{spine-segmentation}.\ The code is inspired heavily by the U-Net implementation created for the official Keras code examples by François Chollet~\cite{image-segmentation-with-a-u-net-like-architecture} but has been adapted to train on the X-ray dataset rather than the original Oxford-IIIT Pet Dataset~\cite{pets-dataset}.\ Importantly, the original implementation is in the form of a Jupyter notebook;\ this had to be converted into a standalone Python application which could be run via a SLURM batch job on the SJSU HPC\@.\ The ReLU activation function, RMSprop optimizer, and Sparse Categorical Crossentropy loss function remain in use exactly as in the original implementation.

    \subsection{Data}\label{subsec:data}

    The authors of the paper that inspired this project~\cite{cobb-angle-measurement-of-spine-from-x-ray-images-using-convolutional-neural-network} kindly shared their data when contacted via email.\ It consists of 481 training images and 128 testing images.\ These raw X-rays vary in size but all provide a posteroanterior view of spines.\ The degree of the curves represented varies from mild to severe.

    Some images exhibit characteristics which present challenges to learning.\ Specifically, the images contain visual artifice in the form of necklaces, organs, and surgically-implanted Harrington rods used to stabilize the spine from further curvature.\ In addition, there is the complication of blurriness which can at times render the spine imperceptibly blended behind visual noise.

    \subsection{Pre-processing}\label{subsec:pre-processing}

    Unfortunately, the provided masks used for image segmentation are in MATLAB format, thereby proving incompatible with this project's Python training implementation.\ Thus, each of the 609 images was hand-labeled into a trimap~\cite{create-trimaps-with-topaz-mask-ai} using the Mask AI program created by Topaz Labs.\ It took 6 hours to label them all.

    The masks were created with red, green, and blue (RGB) regions where green represents the high-confidence segment of the image to learn, red represents the segment to crop, and blue represents the border between the two.\ You can see an example trimap in \autoref{fig:results1}.

    The hand-labeled RGB trimaps required further processing to be used in the U-Net implementation leveraged for this project.\ It expects the trimaps to be constrained to only the three darkest shades of black (1, 2, and 3 in RGB parlance).\ Therefore, the pixels of each RGB trimap were mapped into that colorspace.

    \subsection{Training}\label{subsec:training}

    Training occurred over 15 epochs.\ Higher numbers of epochs proved only to overfit the training data.

    A critical discovery was the impact the batch size played in the accuracy of the model's predictions.\ When left at its initial value of 32, the model struggled to learn how to segment spines due to the drastic reduction in total training data between the pets dataset containing ~7,500 images and the X-ray dataset containing just 481.\ When batch size was reduced to 3, the model's accuracy rose dramatically.

    Performing training on the HPC became a top priority upon discovery of the time required to train the model locally with just a CPU.\ Training the model for 15 epochs took ~25 minutes locally without a GPU.\ Training it on a GPU node in the SJSU HPC yielded a much more reasonable training time of ~2.5 minutes;\ a 10x improvement.

    \begin{figure}
        \caption{Loss and Accuracy}
        \label{fig:loss-and-accuracy}
        \begin{tikzpicture}
            \begin{axis}[
            xlabel={Epoch},
            ylabel={Loss},
            xmin=1, xmax=15,
            ymin=0, ymax=1,
            xtick={1, 3, 6, 9, 12, 15},
            ytick={0, .25, .50, .75, 1},
            legend style={at={(0.5,-0.2)},anchor=north},
            ymajorgrids=true,
            grid style=dashed,
            ]
                \addplot[
                color=blue,
                mark=square
                ]
                coordinates {
                (1, 0.9415369913395908)
                (2, 0.36347173816627926)
                (3, 0.2858551138391097)
                (4, 0.25580175934980315)
                (5, 0.22619957197457552)
                (6, 0.20731205224163002)
                (7, 0.19158202114825448)
                (8, 0.17376136417604154)
                (9, 0.1625804718480342)
                (10, 0.14612651362808216)
                (11, 0.1379509448694686)
                (12, 0.13010555795497364)
                (13, 0.12118976314862569)
                (14, 0.11594720014060537)
                (15, 0.10928217275068164)
                };
                \addplot[
                color=red,
                mark=square
                ]
                coordinates {
                (1, 0.7498944)
                (2, 0.8532715)
                (3, 0.8860656)
                (4, 0.8979578)
                (5, 0.90786415)
                (6, 0.9164315)
                (7, 0.9227066)
                (8, 0.9295607)
                (9, 0.9342024)
                (10, 0.94093925)
                (11, 0.94384027)
                (12, 0.9471116)
                (13, 0.9506346)
                (14, 0.9530575)
                (15, 0.9554094)
                };
                \addplot[
                color=green,
                mark=o
                ]
                coordinates {
                (1, 0.7549296505749226)
                (2, 0.7825332581996918)
                (3, 0.4910720940679312)
                (4, 0.357665928080678)
                (5, 0.3090804498642683)
                (6, 0.27299756929278374)
                (7, 0.31170774064958096)
                (8, 0.2485608123242855)
                (9, 0.508794916793704)
                (10, 0.24579987488687038)
                (11, 0.2450892459601164)
                (12, 0.24228437338024378)
                (13, 0.2619198402389884)
                (14, 0.24979994911700487)
                (15, 0.23182532005012035)
                };
                \addplot[
                color=orange,
                mark=o
                ]
                coordinates {
                (1, 0.66744876)
                (2, 0.6575944)
                (3, 0.79087156)
                (4, 0.8555428)
                (5, 0.86985517)
                (6, 0.88613117)
                (7, 0.8739339)
                (8, 0.9004858)
                (9, 0.81274575)
                (10, 0.901945)
                (11, 0.9038338)
                (12, 0.9087785)
                (13, 0.9017017)
                (14, 0.90983397)
                (15, 0.91657144)
                };
                \addplot[mark=triangle, color=purple]
                coordinates {
                (1, 0.23809038553127024)
                (2, 0.23809038553127024)
                (3, 0.23809038553127024)
                (4, 0.23809038553127024)
                (5, 0.23809038553127024)
                (6, 0.23809038553127024)
                (7, 0.23809038553127024)
                (8, 0.23809038553127024)
                (9, 0.23809038553127024)
                (10, 0.23809038553127024)
                (11, 0.23809038553127024)
                (12, 0.23809038553127024)
                (13, 0.23809038553127024)
                (14, 0.23809038553127024)
                (15, 0.23809038553127024)
                };
                \addplot[mark=triangle, color=black]
                coordinates {
                (1, 0.9196472)
                (2, 0.9196472)
                (3, 0.9196472)
                (4, 0.9196472)
                (5, 0.9196472)
                (6, 0.9196472)
                (7, 0.9196472)
                (8, 0.9196472)
                (9, 0.9196472)
                (10, 0.9196472)
                (11, 0.9196472)
                (12, 0.9196472)
                (13, 0.9196472)
                (14, 0.9196472)
                (15, 0.9196472)
                };
                \legend{Training Loss, Training Accuracy, Validation Loss, Validation Accuracy, Testing Loss, Testing Accuracy}
            \end{axis}
        \end{tikzpicture}
    \end{figure}

    \subsection{Testing}\label{subsec:testing}

    The trained model was evaluated over all 128 testing images plus one extra that will be explained in \autoref{sec:results}.\ Loss and accuracy are reported in~\autoref{fig:loss-and-accuracy} for each data subset.\ Notably, the validation set clearly tracked the out-of-sample testing performance during training.

    \subsection{Post-processing}\label{subsec:post-processing}

    Once the model was trained and evaluated against the testing data, it was used to predict segmentation trimaps for each of the testing images.\ Of course, the predicted trimaps were normalized to the network's output size of 150x150 pixels.\ These square outputs look distorted when compared to the original X-rays, so each was resized to the original dimensions of its respective testing image.

    While input X-rays could now be compared with their predicted trimaps, it became obvious that interlacing the two would yield a superior visualization.\ By leveraging ImageMagick techniques~\cite{image-magick}, each of the testing image trimaps was interlaced with its input image.\ In between, each trimap had to be converted from the predicted representation using three distinguishable colors (black, gray, and white) to an intermediate trimap representation using the three darkest shades of black (1, 2, and 3 in RGB coordinates).\ This conversion produces a trimap that is compatible with ImageMagick, though it is impossible to distinguish between the shades of black visually.\ Each of the final interlaced images was saved to disk.

    \section{Results}\label{sec:results}

    The two metrics used to evaluate the model's ability to learn are loss and accuracy.\ The predictions it made yielded a loss rate of 24\% and an accuracy of 92\% when compared against the hand-labeled trimaps.\ These metrics are likely insufficient for real-world application when compared against human performance.\ While a metric tracking human performance at spine segmentation could not be identified, I expect it to be higher than the model's performance.\ Keep in mind, the ultimate goal of doing Cobb angle prediction has the potential for more competition as inter- and intra-person performance varies between 3--10\%\cite{doi:10.14245/ns.1938426.213} due to factors including image quality, professional experience, and time pressure.\ That problem retains promise for further research.

    The following figures visualize some unique challenges the model faced.\ As you can see in \autoref{fig:results1}, it successfully segments the spine through its extreme curvature, even surviving its encounter with the edge. \autoref{fig:results2} shows how it learns to segment the spine despite the presence of what appear to be Harrington rods surgically implanted to stabilize the spine;\ though, the predicted mask does appear to have included the upper screws of that mechanism. \autoref{fig:results3} and \autoref{fig:results4} both demonstrate failure cases which would prevent Cobb angle prediction.\ In \autoref{fig:results3}, it seems the obscuring organ results in a gap in the mask.\ To human eyes, the organ increases contrast, thus actually making it easier to identify the edges of the spine, but it seems the model cannot make that same conclusion.\ The gap in the predicted mask for \autoref{fig:results4} may also have been affected by an organ, though the necklace may have had a hand to play in the inaccuracy as well.

    Finally, I would like to draw special attention to the predicted mask shown in \autoref{fig:results5}.\ The model has segmented the spine quite well in this case.\ It is especially meaningful to me to see this result as it is, in fact, my own spine.

    \begin{figure}
        \caption{Edge-intersecting spine segmentation}
        \label{fig:results1}
        \centering
        \subfloat[\centering Input]{{\includegraphics[width=2.5cm]{image70-input.jpg} }}
        \subfloat[\centering Trimap]{{\includegraphics[width=2.5cm]{image70-mask.png} }}
        \subfloat[\centering Output]{{\includegraphics[width=2.5cm]{image70-output.png} }}
    \end{figure}

    \begin{figure}
        \caption{Harrington rods slightly impact spine segmentation}
        \label{fig:results2}
        \centering
        \subfloat[\centering Input]{{\includegraphics[width=2.5cm]{image63-input.jpg} }}
        \subfloat[\centering Trimap]{{\includegraphics[width=2.5cm]{image63-mask.png} }}
        \subfloat[\centering Output]{{\includegraphics[width=2.5cm]{image63-output.png} }}
    \end{figure}

    \begin{figure}
        \caption{Obscuring organ results in failed spine segmentation}
        \label{fig:results3}
        \centering
        \subfloat[\centering Input]{{\includegraphics[width=2.5cm]{image32-input.jpg} }}
        \subfloat[\centering Trimap]{{\includegraphics[width=2.5cm]{image32-mask.png} }}
        \subfloat[\centering Output]{{\includegraphics[width=2.5cm]{image32-output.png} }}
    \end{figure}

    \begin{figure}
        \caption{Necklace impacts spine segmentation}
        \label{fig:results4}
        \centering
        \subfloat[\centering Input]{{\includegraphics[width=2.5cm]{image61-input.jpg} }}
        \subfloat[\centering Trimap]{{\includegraphics[width=2.5cm]{image61-mask.png} }}
        \subfloat[\centering Output]{{\includegraphics[width=2.5cm]{image61-output.png} }}
    \end{figure}

    \begin{figure}
        \caption{The author's own spine segmentation}
        \label{fig:results5}
        \centering
        \subfloat[\centering Input]{{\includegraphics[width=2.5cm]{image128-input.jpg} }}
        \subfloat[\centering Trimap]{{\includegraphics[width=2.5cm]{image128-mask.png} }}
        \subfloat[\centering Output]{{\includegraphics[width=2.5cm]{image128-output.png} }}
    \end{figure}

    \section{Conclusion}\label{sec:conclusion}

    The ability to segment the spine from a posteroanterior X-ray with 92\% accuracy is an encouraging first step toward the ultimate goal of Cobb angle prediction.\ Next semester, I intend to build upon this work in the Deep Learning course by training a model to detect the individual vertebrae in segmented spine images.\ For now, I conclude with gratitude to Dr. Mahima Agumbe Suresh for educating me in the fundamentals of machine learning.

    \newpage

    \bibliographystyle{ieeetr}
    \bibliography{report}
\end{document}